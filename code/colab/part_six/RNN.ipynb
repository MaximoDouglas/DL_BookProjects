{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximoDouglas/deep_learning/blob/master/code/colab/part_six/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "co4MgreoOh9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the .csv file\n",
        "breast-cancer-wisconsin.csv"
      ]
    },
    {
      "metadata": {
        "id": "V_j8CUcDIACp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQo5zr_tBbYW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and plot data"
      ]
    },
    {
      "metadata": {
        "id": "qCLKk8u9Bdcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "dataset = pandas.read_csv('international-airline-passengers.csv', usecols=[1], \n",
        "                          engine='python', skipfooter=3)\n",
        "plt.plot(dataset)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9taVERz3D0V0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLP on Airlines Dataset - adapted\n",
        "Train Score: 531.71 MSE (23.06 RMSE)\n",
        "\n",
        "Test Score: 2355.06 MSE (48.53 RMSE)"
      ]
    },
    {
      "metadata": {
        "id": "LEOCeG6TD34s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Multilayer Perceptron to Predict International Airline Passengers (t+1, given t)\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# create and fit Multilayer Perceptron model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=8, input_dim=look_back, activation='relu'))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=2, verbose=2)\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "print('Train Score: %.2f MSE (%.2f RMSE)'% (trainScore, math.sqrt(trainScore)))\n",
        "testScore = model.evaluate(testX, testY, verbose=0)\n",
        "print('Test Score: %.2f MSE (%.2f RMSE)'% (testScore, math.sqrt(testScore)))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ly-ErRqVFGPo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MLP with Window Method on Airline predction problem\n",
        "Train Score: 606.19 MSE (24.62 RMSE)\n",
        "\n",
        "Test Score: 2613.03 MSE (51.12 RMSE)"
      ]
    },
    {
      "metadata": {
        "id": "UCOitNuOFLbs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Multilayer Perceptron to Predict International Airline Passengers (t+1, given t, t-1, t-2)\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "print(len(train), len(test))\n",
        "\n",
        "# reshape dataset\n",
        "look_back = 10\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# create and fit Multilayer Perceptron model\n",
        "model = Sequential()\n",
        "model.add(Dense(units=8, input_dim=look_back, activation='relu'))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=200, batch_size=2, verbose=2)\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "print('Train Score: %.2f MSE (%.2f RMSE)'% (trainScore, math.sqrt(trainScore)))\n",
        "testScore = model.evaluate(testX, testY, verbose=0)\n",
        "print('Test Score: %.2f MSE (%.2f RMSE)'% (testScore, math.sqrt(testScore)))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JyArcQT3I_QL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM on airline data - adapted\n",
        "Train Score: 126.92 RMSE\n",
        "\n",
        "Test Score: 151.53 RMSE"
      ]
    },
    {
      "metadata": {
        "id": "jrUStbwsJB3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM for international airline passengers problem with regression framing\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility25.1. LSTM Network For Regression\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 1\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=4, input_shape=(None, look_back)))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "trainScore = math.sqrt(trainScore)\n",
        "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
        "print('Train Score: %.2f RMSE'% (trainScore))\n",
        "testScore = model.evaluate(testX, testY, verbose=0)\n",
        "testScore = math.sqrt(testScore)\n",
        "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
        "print('Test Score: %.2f RMSE'% (testScore))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWlJ9sIbMIS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM and Window Method on airline data - adapted\n",
        "Train Score: 128.19 RMSE\n",
        "\n",
        "Test Score: 162.05 RMSE"
      ]
    },
    {
      "metadata": {
        "id": "UgXQmGQYMTc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM for international airline passengers problem with window regression framing\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(None, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "trainScore = math.sqrt(trainScore)\n",
        "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
        "print('Train Score: %.2f RMSE'% (trainScore))\n",
        "testScore = model.evaluate(testX, testY, verbose=0)\n",
        "testScore = math.sqrt(testScore)\n",
        "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
        "print('Test Score: %.2f RMSE'% (testScore))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KMnSY7yOmmP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM with timesteps Method on airline data - adapted\n",
        "Train Score: 127.70 RMSE\n",
        "\n",
        "Test Score: 162.77 RMSE"
      ]
    },
    {
      "metadata": {
        "id": "LPtEEz5uOux7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM for international airline passengers problem with time step regression framing\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, input_shape=(look_back, 1)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
        "trainScore = math.sqrt(trainScore)\n",
        "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
        "print('Train Score: %.2f RMSE'% (trainScore))\n",
        "testScore = model.evaluate(testX, testY, verbose=0)\n",
        "testScore = math.sqrt(testScore)\n",
        "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
        "print('Test Score: %.2f RMSE'% (testScore))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w4maX8GkSN6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM with memory (stateful) on airline data - adapted\n",
        "Train Score: 124.80 RMSE\n",
        "\n",
        "Test Score: 159.35 RMSE"
      ]
    },
    {
      "metadata": {
        "id": "9CW4HLpDSQFx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM for international airline passengers problem with memory\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "batch_size = 1\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "for i in range(100):\n",
        "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
        "    model.reset_states()\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, batch_size=batch_size, verbose=0)\n",
        "model.reset_states()\n",
        "trainScore = math.sqrt(trainScore)\n",
        "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
        "print('Train Score: %.2f RMSE'% (trainScore))\n",
        "testScore = model.evaluate(testX, testY, batch_size=batch_size, verbose=0)\n",
        "model.reset_states()\n",
        "testScore = math.sqrt(testScore)\n",
        "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
        "print('Test Score: %.2f RMSE'% (testScore))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
        "testPredict = model.predict(testX, batch_size=batch_size)\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vUVsCmHU_dT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM with stacked LSTM layers - adapted\n",
        "Train Score: 133.84 RMSE\n",
        "\n",
        "Test Score: 183.52 RMSE\n"
      ]
    },
    {
      "metadata": {
        "id": "s0HpJg3UVH1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stacked LSTM for international airline passengers problem with memory\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return numpy.array(dataX), numpy.array(dataY)\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "\n",
        "# load the dataset\n",
        "dataframe = pandas.read_csv('../data/international-airline-passengers.csv', usecols=[1],\n",
        "engine='python', skipfooter=3)\n",
        "dataset = dataframe.values\n",
        "dataset = dataset.astype('float32')\n",
        "\n",
        "# normalize the dataset\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.67)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "\n",
        "# reshape into X=t and Y=t+1\n",
        "look_back = 3\n",
        "trainX, trainY = create_dataset(train, look_back)\n",
        "testX, testY = create_dataset(test, look_back)\n",
        "\n",
        "# reshape input to be [samples, time steps, features]\n",
        "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "\n",
        "# create and fit the LSTM network\n",
        "batch_size = 1\n",
        "model = Sequential()\n",
        "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True,\n",
        "return_sequences=True))\n",
        "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "for i in range(100):\n",
        "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
        "    model.reset_states()\n",
        "\n",
        "# Estimate model performance\n",
        "trainScore = model.evaluate(trainX, trainY, batch_size=batch_size, verbose=0)\n",
        "model.reset_states()\n",
        "trainScore = math.sqrt(trainScore)\n",
        "trainScore = scaler.inverse_transform(numpy.array([[trainScore]]))\n",
        "print('Train Score: %.2f RMSE'% (trainScore))\n",
        "testScore = model.evaluate(testX, testY, batch_size=batch_size, verbose=0)\n",
        "model.reset_states()\n",
        "testScore = math.sqrt(testScore)\n",
        "testScore = scaler.inverse_transform(numpy.array([[testScore]]))\n",
        "print('Test Score: %.2f RMSE'% (testScore))\n",
        "\n",
        "# generate predictions for training\n",
        "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
        "model.reset_states()\n",
        "testPredict = model.predict(testX, batch_size=batch_size)\n",
        "model.reset_states()\n",
        "\n",
        "# shift train predictions for plotting\n",
        "trainPredictPlot = numpy.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = numpy.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = numpy.empty_like(dataset)\n",
        "testPredictPlot[:, :] = numpy.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "\n",
        "# plot baseline and predictions\n",
        "plt.plot(dataset)\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McpHBNGAYZaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM on IMDB data - adapted\n",
        "Accuracy: 86.28%"
      ]
    },
    {
      "metadata": {
        "id": "E1k5AiPoYhGp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "test_split = 0.33\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iWEueqv5aMoI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM on IMDB data with dropout - adapted\n",
        "Accuracy: 85.66%"
      ]
    },
    {
      "metadata": {
        "id": "4MFV6fYFaVgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM with Dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "test_split = 0.33\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length, dropout=0.2))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BgDiV36gcZ0S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM with recurrent dropout - adapted"
      ]
    },
    {
      "metadata": {
        "id": "feQiZl4Rce-E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM with dropout for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "test_split = 0.33\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4TRLCuNWedvi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM and CNN on IMDB data - adapted"
      ]
    },
    {
      "metadata": {
        "id": "GDXUgOlOegv3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# LSTM and CNN for sequence classification in the IMDB dataset\n",
        "import numpy\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 5000\n",
        "test_split = 0.33\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nxlw3CanCJkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simplest LSTM on Alphabet predictions - adapted\n",
        "Model Accuracy: 88.00%"
      ]
    },
    {
      "metadata": {
        "id": "K4bXmmzFCQdr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import np_utils\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# define the raw dataset\n",
        "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "# create mapping of characters to integers (0-25) and the reverse\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 1\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(alphabet) - seq_length, 1):\n",
        "    seq_in = alphabet[i:i + seq_length]\n",
        "    seq_out = alphabet[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(len(alphabet))\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "print(X.shape[1], X.shape[2])\n",
        "\n",
        "# create and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
        "\n",
        "# summarize performance of the model\n",
        "scores = model.evaluate(X, y, verbose=0)\n",
        "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# demonstrate some model predictions\n",
        "for pattern in dataX:\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(len(alphabet))\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    print(seq_in, \"->\", result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bC68ufD2DG7y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM on Alphabet with window method - adapted\n",
        "Acc: 86.96%"
      ]
    },
    {
      "metadata": {
        "id": "e5j0I7ZGDQK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.utils import np_utils\n",
        "from theano.tensor.shared_randomstreams import RandomStreams\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(7)\n",
        "srng = RandomStreams(7)\n",
        "\n",
        "# define the raw dataset\n",
        "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "\n",
        "# create mapping of characters to integers (0-25) and the reverse\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 3\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(alphabet) - seq_length, 1):\n",
        "    seq_in = alphabet[i:i + seq_length]\n",
        "    seq_out = alphabet[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
        "\n",
        "# normalize\n",
        "X = X / float(len(alphabet))\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "# create and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
        "\n",
        "# summarize performance of the model\n",
        "scores = model.evaluate(X, y, verbose=0)\n",
        "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# demonstrate some model predictions\n",
        "for pattern in dataX:\n",
        "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
        "    x = x / float(len(alphabet))\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    print(seq_in, \"->\", result)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}