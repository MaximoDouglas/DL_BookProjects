{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_save_chkpt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximoDouglas/deep_learning/blob/master/code/colab/load_save_chkpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "co4MgreoOh9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the .csv files\n",
        "\n",
        "pima-indians-diabetes_labeled.csv"
      ]
    },
    {
      "metadata": {
        "id": "V_j8CUcDIACp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VATlX4dQO2x8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - JSON - adapted"
      ]
    },
    {
      "metadata": {
        "id": "pVzbmcfbPAIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"001.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"001.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "json_file = open('001.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"001.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GxW3NlO8G9Jh"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - YAML - adapted"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Au0yziN2G9Jl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_yaml\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"002.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"002.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "yaml_file = open('002.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"002.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXcbCINHpVY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving checkpoints\n",
        "Save weights of training improvements"
      ]
    },
    {
      "metadata": {
        "id": "PKVGo4R3pgxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath= \"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}