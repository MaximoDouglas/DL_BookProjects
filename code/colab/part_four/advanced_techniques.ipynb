{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advanced_techniques.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximoDouglas/deep_learning/blob/master/code/colab/advanced_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "co4MgreoOh9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the .csv files\n",
        "\n",
        "pima-indians-diabetes_labeled.csv"
      ]
    },
    {
      "metadata": {
        "id": "V_j8CUcDIACp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VATlX4dQO2x8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - JSON - adapted"
      ]
    },
    {
      "metadata": {
        "id": "pVzbmcfbPAIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"001.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"001.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "json_file = open('001.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"001.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GxW3NlO8G9Jh"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - YAML - adapted"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Au0yziN2G9Jl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_yaml\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"002.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"002.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "yaml_file = open('002.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"002.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXcbCINHpVY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving checkpoints - adapted\n",
        "Save weights of training improvements"
      ]
    },
    {
      "metadata": {
        "id": "PKVGo4R3pgxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath= \"saved_weights/024_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oi0yCk2JrvCi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving the best model on checkpoint - adapted"
      ]
    },
    {
      "metadata": {
        "id": "l3Yg0_22r0wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath= \"saved_weights/025_best_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqvyxfoPtCHM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - loading checkpoint - adapted"
      ]
    },
    {
      "metadata": {
        "id": "1KpLdzbptGTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# How to load and use weights from a checkpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"saved_weights/025_best_weights.hdf5\")\n",
        "\n",
        "# Compile model (required to make predictions)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "og3EuWQO9ss6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving the history plot in a .png figure - Maximo, D.H."
      ]
    },
    {
      "metadata": {
        "id": "7hsGKSgd9xug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize training history\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
        "\n",
        "# list all data in history\n",
        "print(\"Ploted metrics: %r\"%(history.history.keys()))\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','test'], loc=\"lower right\")\n",
        "plt.savefig('plots/028_acc.png')\n",
        "plt.clf()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend(['train','test'], loc=\"upper right\")\n",
        "plt.savefig('plots/028_loss.png')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}