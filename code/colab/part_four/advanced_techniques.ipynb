{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "advanced_techniques.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaximoDouglas/deep-learning-with-python-brownlee/blob/master/code/colab/part_four/advanced_techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "co4MgreoOh9A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Upload the file\n"
      ]
    },
    {
      "metadata": {
        "id": "V_j8CUcDIACp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget \"http://ftp.ics.uci.edu/pub/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\" -O pima-indians-diabetes.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VATlX4dQO2x8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - JSON - adapted"
      ]
    },
    {
      "metadata": {
        "id": "pVzbmcfbPAIR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_json\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"001.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"001.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "json_file = open('001.json','r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"001.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GxW3NlO8G9Jh"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Save and load models - YAML - adapted"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Au0yziN2G9Jl",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MLP for Pima Indians Dataset Serialize to JSON and HDF5\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import model_from_yaml\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Begin data preprocessing\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "# create model\n",
        "# End data preprocessing\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, epochs=150, batch_size=10)\n",
        "\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"Pre-loaded | %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "# Saving model and weights\n",
        "# serialize model to YAML\n",
        "model_yaml = model.to_yaml()\n",
        "with open(\"002.yaml\", \"w\") as yaml_file:\n",
        "    yaml_file.write(model_yaml)\n",
        "\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"002.h5\")\n",
        "# End saving model and weights\n",
        "\n",
        "# Loading model and weights\n",
        "# load model\n",
        "yaml_file = open('002.yaml', 'r')\n",
        "loaded_model_yaml = yaml_file.read()\n",
        "yaml_file.close()\n",
        "loaded_model = model_from_yaml(loaded_model_yaml)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"002.h5\")\n",
        "# End loading model and weights\n",
        "\n",
        "# evaluate loaded model on test data\n",
        "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X, Y)\n",
        "print(\"Loaded | %s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXcbCINHpVY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving checkpoints - adapted\n",
        "Save weights of training improvements"
      ]
    },
    {
      "metadata": {
        "id": "PKVGo4R3pgxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath= \"saved_weights/024_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oi0yCk2JrvCi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Saving the best model on checkpoint - adapted"
      ]
    },
    {
      "metadata": {
        "id": "l3Yg0_22r0wr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint the weights when validation accuracy improves\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath= \"saved_weights/025_best_weights.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqvyxfoPtCHM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - loading checkpoint - adapted"
      ]
    },
    {
      "metadata": {
        "id": "1KpLdzbptGTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# How to load and use weights from a checkpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(8, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
        "model.add(Dense(1, kernel_initializer=\"uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "# load weights\n",
        "model.load_weights(\"saved_weights/025_best_weights.hdf5\")\n",
        "\n",
        "# Compile model (required to make predictions)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# load pima indians dataset\n",
        "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "# estimate accuracy on whole dataset using loaded weights\n",
        "scores = model.evaluate(X, Y)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OyfZPM4gr446",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - dropout on input layer - adapted"
      ]
    },
    {
      "metadata": {
        "id": "YKo5Dvlur-LG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Baseline Model on the Sonar Dataset\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"sonar.data\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# baseline\n",
        "def create_baseline():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dropout(0.2, input_shape=(60,)))\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "estimators = []\n",
        "estimators.append((\"standardize\", StandardScaler()))\n",
        "estimators.append((\"mlp\", KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16)))\n",
        "pipeline = Pipeline(estimators)\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=skfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Cnmt90LtOOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Dropout on hidden layers - adapted"
      ]
    },
    {
      "metadata": {
        "id": "0JQ1z2dvtTjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Baseline Model on the Sonar Dataset\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.constraints import maxnorm\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"sonar.data\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "\n",
        "# baseline\n",
        "def create_baseline():\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=maxnorm(3)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
        "\n",
        "    # Compile model\n",
        "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "estimators = []\n",
        "estimators.append((\"standardize\", StandardScaler()))\n",
        "estimators.append((\"mlp\", KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=16)))\n",
        "pipeline = Pipeline(estimators)\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=skfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQDyKPpNvl_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ionosphere data"
      ]
    },
    {
      "metadata": {
        "id": "Cc8ezMjIvnN5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DO01mF_Ivv7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Time-based learning rate decay - adapted"
      ]
    },
    {
      "metadata": {
        "id": "Z5g_HXObyeFu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Time Based Learning Rate Decay\n",
        "import pandas\n",
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"ionosphere.data\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:34].astype(float)\n",
        "Y = dataset[:,34]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "epochs = 50\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "spdkU1N9e8x-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Code - Drop based learning rate decay - adapted"
      ]
    },
    {
      "metadata": {
        "id": "rqgMXDg1fC4o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drop-Based Learning Rate Decay\n",
        "import pandas\n",
        "import pandas\n",
        "import numpy\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# learning rate schedule\n",
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.1\n",
        "    drop = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lrate\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load dataset\n",
        "dataframe = pandas.read_csv(\"ionosphere.data\", header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:34].astype(float)\n",
        "Y = dataset[:,34]\n",
        "\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# learning schedule callback\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callbacks_list = [lrate]\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}